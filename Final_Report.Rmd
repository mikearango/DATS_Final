---
title: "Using Neighborhood Level Data to Predict the Residential Sale Price of Properties in Ames, Iowa"
author: "Yeshwant Chillakuru, Michael Arango, Jack Crum, Paul Brewster"
date: "4/28/2017"
output:
  html_document:
    highlight: textmate
    theme: yeti
    toc: yes
    fontsize: 11
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r init, include = FALSE, message = FALSE}
# packages needed for modeling
# install.packages("e1071")
# install.packages("ranger")
# install.packages("png")
# install.packages("grid")
# install.packages("glmnet")
# install.packages("kknn")
# install.packages("mboost")
library(tidyverse)
library(stargazer)
library(pls)
library(scales)
library(car)
library(caret)
library(png)
library(grid)
library(lattice)
```

# Intro

The 2008 financial crisis affected the lives of everyone in the United States---some more than others---and there is an abundance of retrospective research that seeks to identify which sectors of the economy were hit hardest during this time and why. The first sector that comes to mind is housing as the financial crisis was "characterized by a rise in subprime mortgage delinquencies and foreclosures, and the resulting decline of securities" backed by these subprime mortgages.[1] For this reason, we decided to use our multifaceted data science toolbox to answer meaningful questions about the housing market. The goal of this project is ultimately to build a regression model to predict defined outcomes. So, while we will not be analyzing the housing market from an economic perspective like the majority of the literature, we can offer up a predictive model for academic purposes. 

As a conceptual framework for our research, we adopted Peng and Matsui's epicycles of analysis to iteratively manage the steps of our data analysis.[2] After choosing the broad topic of the housing market, we narrowed the scope of our research. This included brainstorming as a team to figure out what specific question we wanted to answer about the housing market. We unanimously decided to predict the sale price of a residential house sale based on common characteristics that a property appraiser or assessor's office would use to assess property value. 

## The Dataset

```{r loaddata, echo = FALSE}
df <- read.csv("cleaned_housing.csv", header = TRUE)
df_train <- read.csv("train_housing.csv", header = TRUE)
df_test <- read.csv("test_housing.csv", header = TRUE)
```

The dataset we chose for our project contains information from the Ames Assessorâ€™s Office on residential property sales that occurred in Ames, Iowa from 2006 to 2010.[3] The dataset was published in the *Journal of Statistics Education* for students and researchers alike to have an opportunity to practice predictive modeling on messy, real world data. The dataset contains 2930 observations of 80 variables and the unit of observation is a single property sale in Ames, Iowa in a given year.

Of the 80 variables, 23 are nominal, 23 are ordinal, 14 are discrete, and 20 are continuous. The variables included are basic characteristics that anyone wanting to buy a house would be interested in. For the most part, the different variables may be split up into specific groups. In general, the 20 continuous variables relate to  measurements of area dimensions for each observation. These include, among others, the sizes of lots, rooms, porches, and garages. The 14 discrete variables mostly have to do with the number of bedrooms, bathrooms, kitchens, etc. that a given property has. There are several geographic categorical variables that start profiling properties at the individual Parcel ID level and end at the neighborhood level. The rest of the nominal variables identify characteristics of the property and dwelling type/structure. Most of the ordinal variables are rankings of the quality/condition of rooms and lot characteristics. For more information on the variables in the dataset, consult the included `DataDescription.txt` file.

The dataset was fairly messy with regard to the number of missing values and the way levels of categorical variables were organized and coded. In order to tackle the large amount of cleaning required, we thought it was best to split the data into four equal parts---leaving about 20 variables for each team member to clean. So, we changed all variable names to lowercase as some were upper case to standardize them and divided the variables up accordingly. 

There were 13960 missing values in the dataset when we started to clean and analyze it. *Table 1* breaks down the number of missing values by variable. After analyzing the variables with missing data and consulting the aforementioned `DataDescription.txt` file, we noticed that most of the missing values were actually not missing. The documentation states the data curator coded certain variables as `NA` to specify that a house did not have that feature. For example, in the case of the `alley` variable, `NA` means there is `No Alley` access to the property. By recoding these variables, we were able to fix most of the missing values in categorical variables.  

Variable Name   | Number of Missing Values
----------------|-------------------------
pool.qc         | 2917
misc.feature    | 2824         
alley           | 2732 
fence           | 2358
fireplace.qu    | 1422 
lot.frontage    | 490  
garage.yr.blt   | 159 
garage.qual     | 158
garage.cond     | 158   
garage.type     | 157
garage.finish   | 157   
bsmt.qual       | 79
bsmt.cond       | 79
bsmt.exposure   | 79
bsmtfin.type.1  | 79 
bsmtfin.type.2  | 79 

Table: Table 1: Number of Missing Values by Variable, Number Missing > 75

The solution to fill missing values for quantitative variables was just as simple. Most of the quantitative variables missing large numbers of values were `NA` because a given house feature was missing as well. For example, all missing values in the `garage.yr.blt` variable were because the house does not have a garage. The same reasoning for missing values applies to the square footage variables. For variables that were not missing a large number of variables we used mode (categorical) and median (quantitative) imputation to fill missing values.

Once the variables were cleaned and all missing values were filled, we performed exploratory data analysis of the data. We noticed that the distribution of sale price was heavily skewed to the right. *Figure 1* illustrates this heavy skewness. These high-end outliers will make it very difficult to accurately predict as they are significantly going to drive up the root mean square error (RMSE) of our predictions. Keeping these observations would have disproportionately penalized the predictions of our model. 

```{r graph1, fig.cap = "Figure 1: Histogram of Sale Price Shows Many High-end Outliers", fig.align = 'center', fig.width = 8, fig.height = 5, echo = FALSE}
img <- readPNG("HistSaleprice.png")
grid.raster(img)
```

To remedy this issue, we thought of two possible solutions: 1) drop the outlier observations; or 2) linearly transform the saleprice variable by taking the natural logarithm. We ultimately decided to drop the top and bottom one percent of observations as our goal is to predict *typical* sales in Ames, Iowa. Taking the natural logarithm of the sale price would change the scale of our predictions to a logarithmic scale and would abstract the interpretability of model predictions. In total, we only dropped 58 observations by excluding the top and bottom one percent of sale price. 

The cleaned dataset has 2838 observations of 82 variables. This means that we only dropped 92 observations (3.14% of the original data) throughout the cleaning process. After cleaning the data, we used the `caret` package's `createDataPartition` function to split our cleaned dataset into a train and test set. We decided to split 70 percent of the data into train and the remaining 30 percent into test. The three resulting datasets---cleaned, train, and test---can be found in the included project repository or generated by running the included `Cleaning.rmd` as we set the seed to allow for reproducibility. 

[1]: http://www.stat.unc.edu/faculty/cji/fys/2012/Subprime%20mortgage%20crisis.pdf
[2]: cite the *Art of Data Science Book* here
[3]: *Journal of Statistics Education*, Volume 19, Number 3(2011), accessed April 11, 2017 at www.amstat.org/publications/jse/v19n3/decock.pdf

# Regression Model Approach 1

## Model Creation and Evaluation


```{r, echo=FALSE}
test_y <- df_test[, "saleprice"]

i <- sapply(df, is.factor)
df[i] <- lapply(df[i], as.integer)

i <- sapply(df_train, is.factor)
df_train[i] <- lapply(df_train[i], as.integer)

i <- sapply(df_test, is.factor)
df_test[i] <- lapply(df_test[i], as.integer)
```

We decided to begin our model development process by generating several simple models. Linear models are the first models chosen to examine as they are the easiest to fit. The first model we decided to create was an ordinary least squares (OLS) model containing all the predictors in the data set. 

```{r Full Model, echo=FALSE, results='asis', message=FALSE}
all.model <- lm(saleprice ~ ., data = df_train)
# summary(all.model)
# plot(all.model)
pred.all <- predict(all.model, df_test)
RMSE.all <- sqrt(mean((pred.all - test_y)^2))
# RMSE.all

# alias(all.model, complete = TRUE, partial = FALSE, partial.pattern = FALSE)

stargazer(all.model, type="html", style="apsr")
```

The RMSE of the full model is $`r round(RMSE.all)` The summary shows several instances of perfect multicollinearity, as the error generated by the vif() function shows. Many predictors have high p-values and appear to be statistically insignicant. All predictors that demonstrate perfect multicollinearity or have a p-value > 0.2 are dropped to reduce dimensionality and include only significant variables. The reduced model is run the model again to see if this reduction with improve RMSE.    

```{r Model 2}
model.2 <- lm(saleprice ~ ms.zoning + lot.area + street + land.contour + land.slope + neighborhood + condition.1 + condition.2 + bldg.type + house.style + overall.qual + overall.cond + year.built + roof.style + exterior.1st + exterior.2nd + mas.vnr.type + mas.vnr.area + exter.qual + bsmt.qual + bsmt.exposure + bsmtfin.sf.1 + bsmtfin.sf.2 + bsmt.unf.sf + heating.qc + x1st.flr.sf + x2nd.flr.sf + bsmt.full.bath + bsmt.half.bath + full.bath + bedroom.abvgr + functional + fireplaces + fireplace.qu + garage.yr.blt + garage.cars + garage.area + wood.deck.sf + enclosed.porch + screen.porch + misc.feature + sale.type + sale.condition + ln.lot.frontage + ln.lot.area, data = df_train)
# summary(model.2)
# plot(model.2)
pred.2 <- predict(model.2, df_test)
RMSE.2 <- sqrt(mean((pred.2 - test_y)^2))
# RMSE.2

vif(model.2)
```

The RMSE of reduced model is $`r round(RMSE.2)`, an increase from the full model, and multicollinearity has been reduced as shown by the vif-scores. The adjusted R-squared of the model is 0.8707, demostrating that the model is a good fit of the data. The model is furthur simplified to see if this trend continues as simplifcation increases.

```{r Model 3}
model.3 <- lm(saleprice ~ overall.qual + neighborhood + bldg.type + bsmt.qual + total.bsmt.sf + gr.liv.area + full.bath + half.bath + totrms.abvgrd + kitchen.qual + garage.qual + garage.area + wood.deck.sf + screen.porch + misc.feature, data = df_train)
summary(model.3)
plot(model.3)
pred.3 <- predict(model.3, df_test)
RMSE.3 <- sqrt(mean((pred.3 - test_y)^2))
RMSE.3

vif(model.3)
```

The simplified model showing the simplified model do not fit the models as well as the more complicated linear regression model. The residuals are larger on the ends of the sale price range, demonstrating the poor predictive nature of the linear regression model for the data set. A stepwise regression using backward elimination to run to find the model with the lowest AIC.   

```{r Stepwise Backward}
step.back.model <- step(all.model, direction = "backward")
summary(step.back.model)
pred.test.back <- predict(step.back.model, newdata = df_test)
RMSE.back <- sqrt(mean((pred.test.back - test_y)^2))
RMSE.back
```

The stepwise backwards returns an RMSE = 30399.94 and an adjusted R-squared = 0.8769, . A stepwise regression with forward selection is run to see if this process will create a better fitting model.  

```{r Stepwise Forward}
step.forward.model <- step(all.model, direction = "forward")
summary(step.forward.model)
pred.test.forward <- predict(step.forward.model, newdata = df_test)
RMSE.forward <- sqrt(mean((pred.test.forward - test_y)^2))
RMSE.forward
```

The forward selection model returns an RMSE = 30198.86 and an adjusted R-squared = 0.8764, but contains a number of instances of multicollinearity.

Multiple linear regression has been demonstrated to not be the best method for model creation for this data set. This is a result of assumptions that a linear regression makes about the data that this data set does not conform to. Linear regression models have five assumptions:

  - 1) A linear relationship between the explanatory variables and the response variable
  - 2) Multivariate normality among the explanatory variables
  - 3) Little or no multicollinearity between explanatory variables
  - 4) Little or no auto-correlation
  - 5) Homoscedasticity
  
In this data set, there is no evidence of such a relationship between the predictors and the response variable. The exploratory data analysis demonstrates clear non-normality from the majority of the predictors. This data set also contains high levels of multicollinearity as seen in the full model analysis. The residuals vs. fitted values plot shows that auto-correlation exists in the linear regression models. Finally, as can be clearly seen in the EDA, as each explanatory variable increases in value, the variance increases as well, almost uniformily across the data set, indicating clear heteroscedasticity.

To explore other model creation options, a simple decision tree regression is run and cross-validated. Decision tree regression models are not required to meet the assumptions of a linear regression. The only requirement for decision trees is that the data is splittable, which ideal for the data set.

```{r Decision Tree}
library(tree)

tree_model <- tree(saleprice ~ ., data = df_train, method = "recursive.partition")
plot(tree_model)
text(tree_model, pretty = 0)
tree_pred <- predict(tree_model, df_test)
RMSE.tree <- sqrt(mean((tree_pred - test_y)^2))
RMSE.tree

cv_tree <- cv.tree(tree_model)
plot(cv_tree$size, cv_tree$dev, type = "b", xlab = "Tree Size", ylab = "MSE")
cv_tree$size[1]
```

Pruning is not necessary for the tree as cross-validation plot shows that the maximum number of leafs results in the lowest MSE.

The decision tree model has an RMSE = 36946.74. Although this model has a higher RMSE than the linear and stepwise regressions, the decision tree's lack of assumptions make it a prefered modeling technique for this data. Analyzing the nodes shows that the decision tree only splits into ten leafs and predicts sales prices on very few variables. As such, a more robust decision tree regression model, such as a random forest model, would most like result in a better fitting model.

```{r removeVarsJack, echo=FALSE}
rm(list=setdiff(ls(), c("RMSE.forward", "RMSE.back", "RMSE.tree", "RMSE.1", "RMSE.2", "RMSE.all")))
```

# Regression Model Approach 2
```{r pfb.intro, echo=FALSE, message=FALSE}
train_h.Paul <- read.csv("train_housing.csv", header = TRUE)
test_h.Paul <- read.csv("test_housing.csv", header = TRUE)
```

In this approach to a regression model, we take a systematic approach to generating a regression model used to predict sales prices within the Residential Sales Ames, Iowa dataset. This model serves to better understand the factors that influence sales price and can be considered a form of exploratory data analysis. We first ordered the variables by R^2^ values in relationship to the `salesprice` in order to understand which variables singularly explain the most variance in `saleprice`. 

```{r pfb.building.ordered.list, warning=FALSE, echo=FALSE}
df.Paul <- data.frame(Variable = character(ncol(train_h.Paul)), AdjRSquared = numeric(ncol(train_h.Paul)), stringsAsFactors = FALSE)
for(i in 1:ncol(train_h.Paul)) {
  df.Paul$Variable[i]<- labels(train_h.Paul)[[2]][[i]]
  df.Paul$RSquared[i]<- summary(lm(as.formula(paste("saleprice ~", labels(train_h.Paul)[[2]][[i]])),data=train_h.Paul))$r.squared
}

head(df.Paul[order(df.Paul$RSquared,decreasing=TRUE),],20)
```


## Analysis

### Sales Price and Neighborhood EDA

Based on the order of the variables above, we see that some variables can a greater percentage of the deviation in the saleprice than others. This list suggests that overall quality of the house, neighborhood the house is located, and several other factors are relatively important. We can see from the graph below that `saleprice` varies widely across `neighborhood`, which demonstrate the real estate adage of "location, location, location."

```{r pfb.neighborhood, echo=FALSE}
train_h.Paul$neighborhood <- factor(train_h.Paul$neighborhood, levels = c("MeadowV" ,"IDOTRR" ,"BrDale" ,"OldTown" ,"BrkSide" ,"Edwards" ,"SWISU" ,"Sawyer" ,"NPkVill" ,"Blueste" ,"NAmes" ,"Mitchel" ,"SawyerW" ,"NWAmes" ,"Gilbert" ,"Greens" ,"Blmngtn" ,"CollgCr" ,"Crawfor" ,"ClearCr" ,"Somerst" ,"Timber" ,"Veenker" ,"NridgHt" ,"StoneBr" ,"NoRidge"))
test_h.Paul$neighborhood <- factor(test_h.Paul$neighborhood, levels = c("MeadowV" ,"IDOTRR" ,"BrDale" ,"OldTown" ,"BrkSide" ,"Edwards" ,"SWISU" ,"Sawyer" ,"NPkVill" ,"Blueste" ,"NAmes" ,"Mitchel" ,"SawyerW" ,"NWAmes" ,"Gilbert" ,"Greens" ,"Blmngtn" ,"CollgCr" ,"Crawfor" ,"ClearCr" ,"Somerst" ,"Timber" ,"Veenker" ,"NridgHt" ,"StoneBr" ,"NoRidge"))
ggplot(train_h.Paul, aes(x = neighborhood, y = saleprice)) + 
  geom_boxplot(fill = "#81AC9B") +
  theme(axis.text.x = element_text(angle = 90, size = 8), legend.position = "none") + 
  scale_y_continuous(labels = dollar) 
```

```{r pfb.datawrangling, echo=FALSE}

# In order to better interpret the regression analysis, factor variables are reordered and converted to numeric types relative to their order. For example:
# no fireplace(1) > unfinished fireplace (2) > finished fireplace (3)

test_h.Paul$bsmt.qual = as.numeric(factor(test_h.Paul$bsmt.qual, levels = c("No Basement", "Po", "Fa", "TA", "Gd", "Ex")))
train_h.Paul$bsmt.qual = as.numeric(factor(train_h.Paul$bsmt.qual, levels = c("No Basement", "Po", "Fa", "TA", "Gd", "Ex")))

test_h.Paul$exter.qual = as.numeric(factor(test_h.Paul$exter.qual, levels = c("Po", "Fa", "TA", "Gd", "Ex")))
train_h.Paul$exter.qual = as.numeric(factor(train_h.Paul$exter.qual, levels = c("Po", "Fa", "TA", "Gd", "Ex")))

test_h.Paul$kitchen.qual = as.numeric(factor(test_h.Paul$kitchen.qual, levels = c("Po", "Fa", "TA", "Gd", "Ex")))
train_h.Paul$kitchen.qual = as.numeric(factor(train_h.Paul$kitchen.qual, levels = c("Po", "Fa", "TA", "Gd", "Ex")))

test_h.Paul$garage.finish = as.numeric(factor(test_h.Paul$garage.finish, levels = c("None", "Unf", "RFn", "Fin")))
train_h.Paul$garage.finish = as.numeric(factor(train_h.Paul$garage.finish, levels = c("None", "Unf", "RFn", "Fin")))

test_h.Paul$fireplace.qu = as.numeric(factor(test_h.Paul$fireplace.qu, levels = c("No Fireplace", "Po", "Fa", "TA", "Gd", "Ex")))
train_h.Paul$fireplace.qu = as.numeric(factor(train_h.Paul$fireplace.qu, levels = c("No Fireplace", "Po", "Fa", "TA", "Gd", "Ex")))
```

### Regression Analysis

In order to best understand the influence of variables that each individual explain relatively high portions of the deviation in saleprice, the following multivariate regression models are performed:

```{r pfb.model1, echo=FALSE}
model.Paul<-lm(saleprice~neighborhood +  
    bsmt.qual +  
    total.bsmt.sf +  
    gr.liv.area +  
    exter.qual +   
    kitchen.qual +   
    garage.area +  
    garage.finish +   
    fireplace.qu +  
    yrs.since.remod + 
    ln.lot.area
    , data = train_h.Paul )

model.Paul2<-lm(saleprice~overall.qual +
    neighborhood +  
    bsmt.qual +  
    total.bsmt.sf +  
    gr.liv.area +  
    exter.qual +   
    kitchen.qual +   
    garage.area +  
    fireplace.qu +  
    yrs.since.remod , data = train_h.Paul )

stargazer(model.Paul, model.Paul2, type="text", style="apsr")
```

```{r pfb.testmodel1, echo=FALSE}
p1.Paul <- predict(model.Paul, test_h.Paul)
d.Paul.p1 <- data.frame(p1.Paul, test_h.Paul$saleprice)
saleprice_MSE.1 <- mean((d.Paul.p1[,1]-d.Paul.p1[,2])^2, na.rm=TRUE)
saleprice_RMSE_Paul.1 <- sqrt(saleprice_MSE.1)
```

```{r pfb.testmodel2, echo=FALSE}
Paul2 <- predict(model.Paul2, test_h.Paul)
dPaul.p2 <- data.frame(Paul2, test_h.Paul$saleprice)
saleprice_MSE.1 <- mean((dPaul.p2[,1]-dPaul.p2[,2])^2, na.rm=TRUE)
saleprice_RMSE_Paul.2 <- sqrt(saleprice_MSE.1)
```

From these two models, we can observe that `overall.qual` is significant and positively correlated with saleprice at the 1% level. `Neighborhood` generally has a signifcant impact on sales price in Model 2 at the 5% level. However, some neighborhoods do appear to be uncorrelated with any change in price. All other variables used in the model appear to have sigiificant relationships with `saleprice` at the 1% level.

### Application of Model
Evaluating these multivariate OLS regression model against test data suggest result in fairly low RMSE values: Model 1 = \$`r round(saleprice_RMSE_Paul.1)`, Model 2 = \$`r round(saleprice_RMSE_Paul.2)`. However, these models are likely not strong performers for prediction because of high multicollinearity within the model. For example, `overall.qual` will be highly correlated with `bsmt.qual` and `exter.qual`. In addition, the neighborhoods with higher prices may be highly correlated with `overall.qual`, `bsmt.qual`, and `exter.qual`. The more important application of these models is to develop an understanding for the variables that influence `saleprice`. Models 1 and 2 demonstrate the significance of `neighborhood`, `overall.qual`, `bsmt.qual`, `total.bsmt.sf`, `gr.liv.area`, `exter.qual`, `kitchen.qual`, `garage.area`, `garage.finish`, `fireplace.qual`, `yrs.since.remod`, and `ln.lot.area` on saleprice.


```{r removeVarsPaul, echo=FALSE}
rm(list=setdiff(ls(), c("saleprice_RMSE_Paul.1", "saleprice_RMSE_Paul.2", "RMSE.forward", "RMSE.back", "RMSE.tree", "RMSE.1", "RMSE.2", "RMSE.all")))
```


# Principle Component Regression (PCR) - Yesh

PCR is an application of Principle Component Analysis (PCA) to regression modeling. By creating "components," which are linear transformations of the variables in our dataset, multicollinearity can be minimized by using these components in the regression model, rather than the original variables. This can create a better prediction model, with lower risk of overfitting, than a traditional regression model. However, the ability to interpret the coefficients of the model is lost because each component variable is a linear combination of all the variables in the original dataset.


## Model Generation

```{r dropfactors, echo=FALSE}
df_train <- read.csv("train_housing.csv", header = TRUE)
df_train_num <- df_train
df_test <- read.csv("test_housing.csv", header = TRUE)

for(i in colnames(df_train)){
  if(is.factor( df_train[[i]] )){
    df_train_num[[i]] <- NULL
  }
}
```

Because PCR methods work best with numerical data, we first dropped all factor variables from our training dataset, keeping only integer and numerical datatypes. The dataset now contains `r dim(df_train_num)[2]` variables. We then generate a general pcr model using all the variables in the training dataset to predict saleprice.


```{r modelGeneration}
pcr.saleprice <- pcr(saleprice ~ ., data=df_train_num, scale=TRUE, validation="CV")
```

## Model Selection

In order to select the number of components we want to use for prediction, we examine the RMSE vs Number of Components Plot:
```{r rsmeplot}
validationplot(pcr.saleprice)
```

```{r pcrsummary, include=FALSE}
summary(pcr.saleprice)
```

From the summary and the RSME plot, large jumps in the % of variance explained and RSME are observed from 6 to 7 components, and again from 23 to 24. Three Models will be assessed with 1, 7, and 24 components.

## Model Validation

The table below compares the predicted values to the actual values in the test dataset.

```{r modelvaldiation, echo=FALSE}

pcr.predict.1 <- predict(pcr.saleprice, df_test, ncomp=1)
df_predict.1 <- data.frame(pcr.predict.1, df_test$saleprice)
saleprice_MSE.1 <- mean((df_predict.1[,1]-df_predict.1[,2])^2)
saleprice_RMSE.1 <- sqrt(saleprice_MSE.1)

pcr.predict.2 <- predict(pcr.saleprice, df_test, ncomp=7)
df_predict.2 <- data.frame(pcr.predict.2, df_test$saleprice)
saleprice_MSE.2 <- mean((df_predict.2[,1]-df_predict.2[,2])^2)
saleprice_RMSE.2 <- sqrt(saleprice_MSE.2)

pcr.predict.3 <- predict(pcr.saleprice, df_test, ncomp=24)
df_predict.3 <- data.frame(pcr.predict.3, df_test$saleprice)
saleprice_MSE.3 <- mean((df_predict.3[,1]-df_predict.3[,2])^2)
saleprice_RMSE.3 <- sqrt(saleprice_MSE.3)

stargazer(df_predict.1, df_predict.2, df_predict.3, type="text", header=FALSE)
```

Comparing the 3 Models, we see the following RMSE Values when applying the model to the test dataset:

  * Model 1 (1 Component): $`r paste(round(saleprice_RMSE.1))`
  * Model 2 (7 Component): $`r paste(round(saleprice_RMSE.2))`
  * Model 3 (24 Component): $`r paste(round(saleprice_RMSE.3))`

Model 2 appears to be the best, as Model 3 risks overfitting for not a much better fit.

```{r deleteAllVarYesh, echo=FALSE}
rm(list=setdiff(ls(), c("saleprice_RMSE.1", "saleprice_RMSE.2", "saleprice_RMSE.3", "saleprice_RMSE_Paul.1", "saleprice_RMSE_Paul.2", "RMSE.forward", "RMSE.back", "RMSE.tree", "RMSE.1", "RMSE.2", "RMSE.all")))
```


# Random Forest Regression - Mike

## Data Pre-processing

```{r import and preprocess, include = FALSE}
# read in train and test data
train_mike <- read.csv("train_housing.csv", header = TRUE)
test_mike <- read.csv("test_housing.csv", header = TRUE)
# make a dataframe of all numeric features
num_features = names(which(sapply(train_mike, is.numeric)))
df.numeric = train_mike[num_features]
# make a dataframe of all categorical features
cat_features = names(which(sapply(train_mike, is.factor)))
# convert all categorical features to numeric variables for modeling
train_mike[cat_features] <- sapply(train_mike[cat_features], as.integer)
test_mike[cat_features] <- sapply(test_mike[cat_features], as.integer)
# split test into x and y
test_x <- test_mike %>% dplyr::select(-saleprice)
test_y <- test_mike %>% dplyr::select(saleprice)
```

Most complex machine learning models do better if we convert factors to integers when we run models, so we converted all categorical variables to integers. Then we created our own folds to be used in k-fold cross validation (with k = 5) so that we can compare the out-of-sample RMSE of our trained models on the *same* cross validation folds. This ensures we have an apples-to-apples comparison of our models. In addition to making our own folds, we created a custom `trainControl` object so we can specify to use these folds.

```{r folds}
# set seed for reproducibility
set.seed(256)
# create 5 folds to be used in cross validation
myFolds <- createFolds(train_mike, k = 5)
# create a custom trainControl object to use our folds; index = myFolds
myControl = trainControl(verboseIter = FALSE, index = myFolds)
```

## Modeling

This section deals with some slightly more complicated prediction models. We ran a few random forest regression models, followed by two elastic net regularization models, and finished off with two more specialized models (*k*-NN regression and gradient boosting). 

### Random Forest

Random forests are aggregations of several decision tree models. A single decision tree is very fast but not too accurate. The earlier comparisons between the decision tree and a simple linear model illustrated this. While random forests take longer to run, they are very accurate and robust to overfitting. Random forests improve accuracy by fitting many trees to a bootstrap sample of the original dataset. This is a technique called *bootstrap aggregation* or *bagging*. In addition to making trees by bagging, "random forests change how the classification or regression trees are constructed. In standard trees, each node is split using the best split among all variables. In a random forest, each node is split using the best among a subset of predictors randomly chosen at that node."[4] 

[4]:http://www.bios.unc.edu/~dzeng/BIOS740/randomforest.pdf 

```{r rf1}
set.seed(12)
# train the model 
modelrf1 <- train(
  # formula
  saleprice ~ .,
  # data
  train_mike,
  # fast random forest
  method = "ranger",
  importance = "impurity",
  # grid search for optimal number of columns to randomly sample at each split
  tuneGrid = data.frame(mtry = seq(2, 80, 2)),
  # set trainCrol as our custom object
  trControl = myControl
)
# print the model
print(modelrf1)
```

We started by running a kitchen sink model (includes all independent variables) since random forest does its own feature selection. Random forest is simple because it only has two hyperparameters we need to worry about---the number of variables in the random subset at each node and the number of trees in the forest. We decided to run a grid search to find the optimal number of variables to include at each split, while leaving the number of trees constant at 500 (the default value). *Figure X* shows the optimal number of predictors at each split (mtry) was 62. Using the optimal number of predictors, the model yields a training RMSE of \$`r (min(modelrf1$results[['RMSE']]))` and the importance of the top 20 predictor variables is shown in *Figure Y*.  

```{r plotrf1, fig.cap = "Figure X: Training RMSE of Random Forest by Number of Predictors (mtry)", fig.align = 'center', echo = FALSE}
# plot the number of randomly selected predictors
plot(modelrf1)
```

```{r plotrf2, fig.cap = "Figure Y: Top 20 Independent Variables by Importance", fig.align = 'center', echo = FALSE}
# plot 20 most important predictors of modelrf1
plot(varImp(modelrf1), top = 20)
```

Next, we decided to remove the zero variance predictors before running the next random forest regression to see if this results in a lower training RMSE. Since we removed zero variance predictors, the number of variables in the dataset decreased. Thus, we had to run a new grid search when we ran the second random forest model. But, it's worth noting that the regression output shows no variables were actually removed after pre-processing.
Regardless, these modifications made the model slightly worse and increased the RMSE to $46327.18. We hypothesize that this is because we ran a smaller grid search since we would expect a handful of variables to be zero variance predictors in a dataset this large.  

```{r rf2, echo = FALSE}
set.seed(1760)
# train the model 
modelrf2 <- train(
  # formula
  saleprice ~ .,
  # data
  train_mike,
  # fast random forest
  method = "ranger",
  # grid search for optimal number of columns to randomly sample at each split
  tuneGrid = data.frame(mtry = seq(2, 40, 2)),
  # preprocess by removing zero variance predictors
  preProcess = "zv",
  # set trainCrol as our custom object
  trControl = myControl
)
```

In an effort to reduce the dimensionality of the dataset, we decided to drop near-zero variance predictors, center and scale all remaining variables, and perform a Principal Components Analysis (PCA) before feeding the data into the third random forest model. 

```{r rf3, echo = FALSE}
set.seed(17)
# train the model 
modelrf3 <- train(
  # formula
  saleprice ~ .,
  # data
  train_mike,
  # fast random forest
  method = "ranger",
  # grid search for optimal number of columns to randomly sample at each split
  tuneGrid = data.frame(mtry = seq(1, 10, 1)),
  # preProcess
  preProcess = c("nzv", "center", "scale", "pca"),
  # set trainCrol as our custom object
  trControl = myControl
)
# print the model summary
print(modelrf3)
```

Surprisingly, this model had the highest training RMSE of the three models run to this point (\$`r min(modelrf3$results$RMSE)`). The regression output shows that 19 variables were removed because they were near-zero variance predictors and the remaining 62 were centered, scaled, and subject to a PCA. This model also explained the least amount of variation in sale price. 

## Elastic Net Regularization 

Elastic net regularization is a mixed method model that is closely linked to lasso (least absolute shrinkage and selection operator) and ridge regression. Lasso and ridge regression are two very common ways of dealing with collinearity of variables and can be seen as a subsitute to stepwsie regression and dimensionality reduction techniques. However, each method deals with collinearity differently. Lasso regression penalizes the number of non-zero coefficients whereas ridge regression penalizes the absolute magnitude of the coefficients. Elastic net regularization is a combination of the two models with a parameter $\alpha$, bounded between zero and one, that is referred to as the *mixing* parameter---0 is pure-lasso regression and 1 is pure-ridge regression. Elastic net also has a $\lambda$ paramater which ranges from zero to infinity which is referred to as the *regularization*---this parameter represents the size of the penalty and higher values of lambda correspond to simpler models.[5]

[5]: https://www.mathworks.com/help/stats/lasso-and-elastic-net.html

```{r enet1}
set.seed(500)
# Train glmnet with custom trainControl and tuning: model
enet1 <- train(
  # formula
  saleprice ~ ., 
  # data
  train_mike,
  # set grid search parameters for alpha
  tuneGrid = expand.grid(alpha = seq(0,1,0.1), 
                         lambda = seq(0.0001, 1, 10)),
  # use glmnet method for lasso, ridge, and elastic net 
  method = "glmnet",
  # trainControl
  trControl = myControl
)
# Print model output to console
print(enet1)
```

For the first model, we decided to run a grid search for the optimal *mixing* parameter while holding the penalty factor constant. *Figure Z* shows the optimal value of $\alpha$ was one for the grid search we ran. This implies that pure ridge regression will minimize the RMSE of our predictions for the regularization models. Note that we only run a grid search for $0 \leq \alpha \leq 1$ by steps of 0.1. That is, it is entirely possible that $\alpha_{optimal} \in (0.9,1)$ but it would have been more computationally expensive to perform a more robust grid search. Ridge regression results in the lowest training RMSE of the complex models (\$`r min(enet1$results$RMSE)`), but this is before we even try to find the RMSE minimizing value of the regularization parameter. 

```{r enet plot, fig.cap = "Figure Z: RMSE of Elastic Net Regularization for Varying Levels of the Mixing Parameter", fig.align = 'center', echo = FALSE}
# plot RMSE vs alpha
plot(enet1)
```

Since we found our optimal value of the mixing parameter to be one, we run a pure-ridge regression model as the second regularization model. So, holding $\alpha$ constant and equal to one, we performed a grid search for the RMSE minimizing value of the regularization parameter. Again, a very robust grid search would be computationally expensive, so we performed a grid search for $\lambda \in [0,1000]$ by steps of 1000. 

```{r ridge}
set.seed(1267)
# Train glmnet with custom trainControl and tuning: model
ridge <- train(
  # formula
  saleprice ~ ., 
  # data
  train_mike,
  # set grid search parameters for lambda
  tuneGrid = expand.grid(alpha = 1, 
                         lambda = (0:15) * 1000),
  # use glmnet method for lasso, ridge, and elastic net 
  method = "glmnet",
  # trainControl
  trControl = myControl
)
# Print model output to console
print(ridge)
```

The regression results show that the optimal model drops our training RMSE down to \$`r min(ridge$results$RMSE)` which is significantly better than before we performed a grid search for the regularization parameter. Below, *Figure A* shows that $\lambda_{optimal} = 7000$. We can assume that we have at least found a local minimum for $\lambda_{optimal}$ as the figure shows a U-shaped curve for the regularization parameter and the slope of the curve is equal to zero at $\lambda = 7000$. 

```{r ridge plot, fig.cap = "Figure Z: RMSE of Elastic Net Regularization for Varying Levels of the Mixing Parameter", fig.align = 'center', echo = FALSE}
# plot RMSE vs lambda
plot(ridge)
```

## k-Nearest Neighbors Regression and Gradient Boosting

The *k*-nearest neighbors algorithm is a non-parametric method that can be used for classification or regression. We take the *k* closest examples to the point we are trying to predict via a given distance metric (euclidean, manhattan, minkowski, etc.) and fit the best line between the neighbors. This process is repeated for each point in the model, so it is a technique that is very computationally expensive if used on a larger dataset. It is important to find the right value of *k* as we can easily overfit by picking a *k* that is too low or underfit by picking a *k* that is too high. 

```{r knn}
set.seed(579)
# Train glmnet with custom trainControl and tuning: model
knn1 <- train(
  # formula
  saleprice ~ ., 
  # data
  train_mike,
  # knn regression
  method = "kknn",
  # trainControl
  trControl = myControl
)
# Print model to console
print(knn1)
```

Most of the literature uses a value of *k* between three and nine, but we ran a grid search between five and nine for the optimal value of *k*.The training RMSE for *k*-nearest neighbor regression was \$`r min(knn1$results$RMSE)` which is higher than most of our models. *Figure B* shows that $k_{max} = 7$ is the value of *k* that optimizes the results of the model on the training data. 

```{r knn plot, fig.cap = "Figure B: RMSE for Varying Numbers of Maximum Neighbors", fig.align = 'center', echo = FALSE}
plot(knn1)
```

The last model we decided to run is a gradient boosting linear model that uses penalized least squares regression to optimize the RMSE of our model on the training data. This model starts by fitting an additive model in a forward stepwise manner (similar to a decision tree) and uses the concept of *gradient descent*---minimizing a function by moving in the opposite direction of the gradient---to add new inputs to the model. For the boosting model, we decided not to *prune* our boosted trees and to find the optimal number of boosting iterations (mstop) between between 10 and 150 by increments of 10. 

```{r glmboost}
glmBoostGrid1 = expand.grid(mstop = seq(10, 150, 10),
                           prune = c("no"))
set.seed(1254)
# Train glmnet with custom trainControl and tuning: model
glmboost1 <- train(
  # formula
  saleprice ~ ., 
  # data
  train_mike,
  tuneGrid = glmBoostGrid1,
  # gradient boosting tree
  method = "glmboost",
  # trainControl
  trControl = myControl
)
# Print model to console
print(glmboost1)
# plot RMSE vs alpha
plot(glmboost1)
```

The gradient boosted tree gave us a marginally lower RMSE (\$`r min(glmboost1$results$RMSE)`) than the optimized ridge regression model. *Figure C* shows the number optimal number of boosting iterations for this model was 40. It would be ideal if the gradient boosted tree model ends up having the lowest RMSE on the training data as well since the coefficients are easily interpretable just as in OLS regression. 

## Model Comparison

To compare models in which we cross-validated on the same folds, we decided make a list of all the models and their results. Then we used the `resamples` function in the `caret` package to take bootstrap samples of the model results. This allows us to make statistical inferences about the performance differences between the models. Below, *Figure D* shows a box-and-whisker plot of the RMSE of the bootstrap samples of our models and *Figure E* shows a box-and-whisker plot of the $R^2$ of the bootstrap samples. 

```{r model list, echo = FALSE}
# Create model_list
model_list <- list(rf1= modelrf1, rf2 = modelrf2, rf3 = modelrf3, `elastic net` = enet1, 
                   ridge = ridge, knn = knn1, glmboost = glmboost1)
# Pass model_list to resamples(): resamples
resamps <- resamples(model_list)
# Summarize the results
summary(resamps)
```

```{r rmse models, fig.cap = "Figure D: RMSE of Bootstrap Samples of The Trained Models", fig.align = 'center', echo = FALSE}
bwplot(resamps, metric = "RMSE")
```


```{r r2models, fig.cap = "Figure E: R-squared of Bootstrap Samples of The Trained Models", fig.align = 'center', echo = FALSE}
bwplot(resamps, metric = "Rsquared")
```

## Testing

```{r rmse function, echo = FALSE}
# A function to calculate the RMSE for a given model. The function takes 2 inputs: prediction and actual
# prediction should be a vector of predicted values and actual should be the vector of actual values
rmse <- function(prediction, actual) {
  # calculate error
  error <- prediction - actual
  # calculate RMSE
  sqrt(mean(error^2))
}
```

```{r calculate predictions rmse, echo = FALSE}
# Calculate model predictions on test data
pred_rf1 <- predict(modelrf1, test_x)
pred_rf2 <- predict(modelrf2, test_x)
pred_rf3 <- predict(modelrf3, test_x)
pred_enet <- predict(enet1, test_x)
pred_ridge <- predict(ridge, test_x)
pred_knn <- predict(knn1, test_x)
pred_glmboost <- predict(glmboost1, test_x)
# Calculate test RMSE 
RMSE_rf1 <- rmse(pred_rf1, test_y)
RMSE_rf2 <- rmse(pred_rf2, test_y)
RMSE_rf3 <- rmse(pred_rf3, test_y)
RMSE_enet <- rmse(pred_enet, test_y)
RMSE_ridge <- rmse(pred_ridge, test_y)
RMSE_knn <- rmse(pred_knn, test_y)
RMSE_glmboost <- rmse(pred_glmboost, test_y)
```

We calculated the predicted values of each of our models on the test dataset and calculated the RMSE of each. The results can be seen in *Table 2* below. 

Model Name      | Test RMSE
----------------|-------------------------
rf1             | `r RMSE_rf1`
rf2             | `r RMSE_rf2`         
rf3             | `r RMSE_rf3`
elastic net     | `r RMSE_enet`
ridge           | `r RMSE_ridge` 
k-NN            | `r RMSE_knn` 
glmboost        | `r RMSE_glmboost`

Table: Table 2: Test RMSE of Machine Learning Models

