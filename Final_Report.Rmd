---
title: "Using Neighborhood Level Data to Predict the Residential Sale Price of Properties in Ames, Iowa"
author: "Yeshwant Chillakuru, Michael Arango, Jack Crum, Paul Brewster"
date: "4/28/2017"
output:
  html_document:
    highlight: textmate
    theme: yeti
    toc: yes
    fontsize: 11
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(stargazer)
library(pls)
```

# Intro

The 2008 financial crisis affected the lives of everyone in the United States---some more than others---and there is an abundance of retrospective research that seeks to identify which sectors of the economy were hit hardest during this time and why. The first sector that comes to mind is housing as the financial crisis was "characterized by a rise in subprime mortgage delinquencies and foreclosures, and the resulting decline of securities" backed by these subprime mortgages.[1] For this reason, we decided to use our multifaceted data science toolbox to answer meaningful questions about the housing market. The goal of this project is ultimately to build a regression model to predict defined outcomes. So, while we will not be analyzing the housing market from an economic perspective like the majority of the literature, we can offer up a predictive model for academic purposes. 

As a conceptual framework for our research, we adopted Peng and Matsui's epicycles of analysis to iteratively manage the steps of our data analysis.[2] After choosing the broad topic of the housing market, we narrowed the scope of our research. This included brainstorming as a team to figure out what specific question we wanted to answer about the housing market. We unanimously decided to predict the sale price of a residential house sale based on common characteristics that a property appraiser or assessor's office would use to assess property value. 

**Maybe add small lit review section here**

## The Dataset

```{r loaddata}

df <- read.csv("cleaned_housing.csv", header=TRUE)
df_train <- read.csv("train_housing.csv", header=TRUE)
df_test <- read.csv("test_housing.csv", header=TRUE)

```


The dataset we chose for our project contains information from the Ames Assessorâ€™s Office on residential property sales that occurred in Ames, Iowa from 2006 to 2010.[3] The dataset was published in the *Journal of Statistics Education* for students and researchers alike to have an opportunity to practice predictive modeling on messy, real world data. The dataset contains 2930 observations of 80 variables and the unit of observation is a single property sale in Ames, Iowa in a given year.

Of the 80 variables, 23 are nominal, 23 are ordinal, 14 are discrete, and 20 are continuous. The variables included are basic characteristics that anyone wanting to buy a house would be interested in. For the most part, the different variables may be split up into specific groups. In general, the 20 continuous variables relate to  measurements of area dimensions for each observation. These include, among others, the sizes of lots, rooms, porches, and garages. The 14 discrete variables mostly have to do with the number of bedrooms, bathrooms, kitchens, etc. that a given property has. There are several geographic categorical variables that start profiling properties at the individual Parcel ID level and end at the neighborhood level. The rest of the nominal variables identify characteristics of the property and dwelling type/structure. Most of the ordinal variables are rankings of the quality/condition of rooms and lot characteristics. For more information on the variables in the dataset, consult the included `DataDescription.txt` file.

**Continue talking about the data cleaning here**


[1]: http://www.stat.unc.edu/faculty/cji/fys/2012/Subprime%20mortgage%20crisis.pdf
[2]: cite the *Art of Data Science Book* here
[3]: *Journal of Statistics Education*, Volume 19, Number 3(2011), accessed April 11, 2017 at www.amstat.org/publications/jse/v19n3/decock.pdf

# Regression Model Approach 1 - Jack

# Regression Model Approach 2 - Paul

# Principle Component Regression (PCR) - Yesh

The second best model for predicting `saleprice`, as defined by the RMSE value on the test data set was the PCR model with 7 components. PCR is an application of Principle Component Analysis (PCA) to regression modeling. By creating "components," which are linear transformations of the variables in our dataset, multicollinearity can be minimized by using these components in the regression model, rather than the original variables. This can create a better prediction model, with lower risk of overfitting, than a traditional regression model. However, the ability to interpret the coefficients of the model is lost because each component variable is a linear combination of all the variables in the original dataset.


## Model Generation

Because PCR methods work best with numerical data, we first dropped all factor variables from our training dataset, keeping only integer and numerical datatypes
```{r dropfactors}
df_train_num <- df_train

for(i in colnames(df_train)){
  if(is.factor( df_train[[i]] )){
    df_train_num[[i]] <- NULL
  }
}

```

The dataset now contains `r dim(df_train_num)[2]` variables. We then generate a general pcr model using all the variables in the training dataset to predict saleprice.
```{r modelGeneration}
pcr.saleprice <- pcr(saleprice ~ ., data=df_train_num, scale=TRUE, validation="CV")
```

## Model Selection

In order to select the number of components we want to use for prediction, we examine the RMSE vs Number of Components Plot:
```{r rsmeplot}
validationplot(pcr.saleprice)
```

And we examine the summary of the model `pcr.saleprice`:

```{r pcrsummary}
summary(pcr.saleprice)
```

From the summary and the RSME plot, large jumps in the % of variance explained and RSME are observed from 6 to 7 components, and again from 23 to 24. Three Models will be assessed with 1, 7, and 24 components.

## Model Validation

The table below compares the predicted values to the actual values in the test dataset.

```{r modelvaldiation, echo=FALSE}

pcr.predict.1 <- predict(pcr.saleprice, df_test, ncomp=1)
df_predict.1 <- data.frame(pcr.predict.1, df_test$saleprice)
saleprice_MSE.1 <- mean((df_predict.1[,1]-df_predict.1[,2])^2)
saleprice_RMSE.1 <- sqrt(saleprice_MSE.1)

pcr.predict.2 <- predict(pcr.saleprice, df_test, ncomp=7)
df_predict.2 <- data.frame(pcr.predict.2, df_test$saleprice)
saleprice_MSE.2 <- mean((df_predict.2[,1]-df_predict.2[,2])^2)
saleprice_RMSE.2 <- sqrt(saleprice_MSE.2)

pcr.predict.3 <- predict(pcr.saleprice, df_test, ncomp=24)
df_predict.3 <- data.frame(pcr.predict.3, df_test$saleprice)
saleprice_MSE.3 <- mean((df_predict.3[,1]-df_predict.3[,2])^2)
saleprice_RMSE.3 <- sqrt(saleprice_MSE.3)

stargazer(df_predict.1, df_predict.2, df_predict.3, type="text")
```


Comparing the 3 Models, we see the following RMSE Values when applying the model to the test dataset:

  * Model 1 (1 Component): `r paste(round(saleprice_RMSE.1))`
  * Model 2 (7 Component): `r paste(round(saleprice_RMSE.2))`
  * Model 3 (24 Component): `r paste(round(saleprice_RMSE.3))`

Model 2 appears to be the best, as Model 3 risks overfitting for not a much better fit.


# Random Forest Regression - Mike








