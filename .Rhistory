for(i in colnames(df)){
if(is.factor( df[[i]] )){
df_num[[i]] <- NULL
}
}
str(df_num)
summary(df$saleprice)
pcr.saleprice <- pcr(saleprice ~ ., data=df_num, scale=TRUE, validation="CV")
validationplot(pcr.saleprice)
summary(pcr.saleprice)
pcr.predict.1 <- predict(pcr.saleprice, df_test, ncomp=1)
df_predict.1 <- data.frame(pcr.predict.1, df_test$saleprice)
saleprice_MSE.1 <- mean((df_predict.1[,1]-df_predict.1[,2])^2)
saleprice_RMSE.1 <- sqrt(saleprice_MSE.1)
pcr.predict.2 <- predict(pcr.saleprice, df_test, ncomp=7)
df_predict.2 <- data.frame(pcr.predict.2, df_test$saleprice)
saleprice_MSE.2 <- mean((df_predict.2[,1]-df_predict.2[,2])^2)
saleprice_RMSE.2 <- sqrt(saleprice_MSE.2)
pcr.predict.3 <- predict(pcr.saleprice, df_test, ncomp=24)
df_predict.3 <- data.frame(pcr.predict.3, df_test$saleprice)
saleprice_MSE.3 <- mean((df_predict.3[,1]-df_predict.3[,2])^2)
saleprice_RMSE.3 <- sqrt(saleprice_MSE.3)
stargazer(df_predict.1, df_predict.2, df_predict.3, type="text")
paste(round(saleprice_RMSE.1))
paste(round(saleprice_RMSE.2))
paste(round(saleprice_RMSE.3))
paste(round(saleprice_RMSE.2))
paste(round(saleprice_RMSE.3))
paste(round(saleprice_RMSE.1))
paste(round(saleprice_RMSE.2))
paste(round(saleprice_RMSE.3))
summary(pcr.saleprice)
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(pls)
library(stargazer)
df <- read.csv("../train_housing.csv", header=TRUE)
df_test <- read.csv("../test_housing.csv", header=TRUE)
df_num <- df
for(i in colnames(df)){
if(is.factor( df[[i]] )){
df_num[[i]] <- NULL
}
}
str(df_num)
summary(df$saleprice)
pcr.saleprice <- pcr(saleprice ~ ., data=df_num, scale=TRUE, validation="CV")
validationplot(pcr.saleprice)
summary(pcr.saleprice)
pcr.predict.1 <- predict(pcr.saleprice, df_test, ncomp=1)
df_predict.1 <- data.frame(pcr.predict.1, df_test$saleprice)
saleprice_MSE.1 <- mean((df_predict.1[,1]-df_predict.1[,2])^2)
saleprice_RMSE.1 <- sqrt(saleprice_MSE.1)
pcr.predict.2 <- predict(pcr.saleprice, df_test, ncomp=7)
df_predict.2 <- data.frame(pcr.predict.2, df_test$saleprice)
saleprice_MSE.2 <- mean((df_predict.2[,1]-df_predict.2[,2])^2)
saleprice_RMSE.2 <- sqrt(saleprice_MSE.2)
pcr.predict.3 <- predict(pcr.saleprice, df_test, ncomp=10)
df_predict.3 <- data.frame(pcr.predict.3, df_test$saleprice)
saleprice_MSE.3 <- mean((df_predict.3[,1]-df_predict.3[,2])^2)
saleprice_RMSE.3 <- sqrt(saleprice_MSE.3)
stargazer(df_predict.1, df_predict.2, df_predict.3, type="text")
paste(round(saleprice_RMSE.3))
paste(round(saleprice_RMSE.2))
paste(round(saleprice_RMSE.3))
paste(round(saleprice_RMSE.2))
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(pls)
library(stargazer)
df <- read.csv("../train_housing.csv", header=TRUE)
df_test <- read.csv("../test_housing.csv", header=TRUE)
df_num <- df
for(i in colnames(df)){
if(is.factor( df[[i]] )){
df_num[[i]] <- NULL
}
}
str(df_num)
summary(df$saleprice)
pcr.saleprice <- pcr(saleprice ~ ., data=df_num, scale=TRUE, validation="CV")
validationplot(pcr.saleprice)
summary(pcr.saleprice)
pcr.predict.1 <- predict(pcr.saleprice, df_test, ncomp=1)
df_predict.1 <- data.frame(pcr.predict.1, df_test$saleprice)
saleprice_MSE.1 <- mean((df_predict.1[,1]-df_predict.1[,2])^2)
saleprice_RMSE.1 <- sqrt(saleprice_MSE.1)
pcr.predict.2 <- predict(pcr.saleprice, df_test, ncomp=7)
df_predict.2 <- data.frame(pcr.predict.2, df_test$saleprice)
saleprice_MSE.2 <- mean((df_predict.2[,1]-df_predict.2[,2])^2)
saleprice_RMSE.2 <- sqrt(saleprice_MSE.2)
pcr.predict.3 <- predict(pcr.saleprice, df_test, ncomp=24)
df_predict.3 <- data.frame(pcr.predict.3, df_test$saleprice)
saleprice_MSE.3 <- mean((df_predict.3[,1]-df_predict.3[,2])^2)
saleprice_RMSE.3 <- sqrt(saleprice_MSE.3)
stargazer(df_predict.1, df_predict.2, df_predict.3, type="text")
getwd()
getwd()
knitr::opts_chunk$set(echo = TRUE)
# packages needed for modeling
# install.packages("e1071")
# install.packages("ranger")
# install.packages("png")
# install.packages("grid")
# install.packages("glmnet")
# install.packages("kknn")
# install.packages("mboost")
# install.packages("htmlTable")
library(tidyverse)
library(stargazer)
library(pls)
library(scales)
library(car)
library(caret)
library(png)
library(grid)
library(lattice)
library(e1071)
library(ranger)
library(htmlTable)
options(scipen = 999)
df <- read.csv("cleaned_housing.csv", header = TRUE)
df_train <- read.csv("train_housing.csv", header = TRUE)
df_test <- read.csv("test_housing.csv", header = TRUE)
img <- readPNG("HistSaleprice.png")
grid.raster(img)
# specify actual values in test set for model predictions
test_y <- df_test[, "saleprice"]
# convert factors to integers in df
i <- sapply(df, is.factor)
df[i] <- lapply(df[i], as.integer)
# convert factors to integers in train set
i <- sapply(df_train, is.factor)
df_train[i] <- lapply(df_train[i], as.integer)
# convert factors to integers in test set
i <- sapply(df_test, is.factor)
df_test[i] <- lapply(df_test[i], as.integer)
# kitchen sink model
all.model <- lm(saleprice ~ ., data = df_train)
# predict on test set
pred.all <- predict(all.model, df_test)
RMSE.all <- sqrt(mean((pred.all - test_y)^2))
# linear model 2
model.2 <- lm(saleprice ~ ms.zoning + lot.area + street + land.contour + land.slope + neighborhood + condition.1 + condition.2 + bldg.type + house.style + overall.qual + overall.cond + year.built + roof.style + exterior.1st + exterior.2nd + mas.vnr.type + mas.vnr.area + exter.qual + bsmt.qual + bsmt.exposure + bsmtfin.sf.1 + bsmtfin.sf.2 + bsmt.unf.sf + heating.qc + x1st.flr.sf + x2nd.flr.sf + bsmt.full.bath + bsmt.half.bath + full.bath + bedroom.abvgr + functional + fireplaces + fireplace.qu + garage.yr.blt + garage.cars + garage.area + wood.deck.sf + enclosed.porch + screen.porch + misc.feature + sale.type + sale.condition + ln.lot.frontage + ln.lot.area, data = df_train)
# calculate rmse
pred.2 <- predict(model.2, df_test)
RMSE.2 <- sqrt(mean((pred.2 - test_y)^2))
# calculate variance inflation factors
vif(model.2)
# linear model 3
model.3 <- lm(saleprice ~ overall.qual + neighborhood + bldg.type + bsmt.qual + total.bsmt.sf + gr.liv.area + full.bath + half.bath + totrms.abvgrd + kitchen.qual + garage.qual + garage.area + wood.deck.sf + screen.porch + misc.feature, data = df_train)
# Calculate RMSE on test set
pred.3 <- predict(model.3, df_test)
RMSE.3 <- sqrt(mean((pred.3 - test_y)^2))
stargazer(all.model, model.2, model.3, type = "html", style = "apsr", title=
"Table 2: Simple OLS Regressions", digits=2)
img <- readPNG("overallquality.png")
grid.raster(img)
plot(model.3)
rm(list = setdiff(ls(), c("RMSE.1", "RMSE.2", "RMSE.all")))
train_h.Paul <- read.csv("train_housing.csv", header = TRUE)
test_h.Paul <- read.csv("test_housing.csv", header = TRUE)
# compute and sort by r-squared
df.Paul <- data.frame(Variable = character(ncol(train_h.Paul)), stringsAsFactors = FALSE)
for(i in 1:ncol(train_h.Paul)) {
df.Paul$Variable[i]<- labels(train_h.Paul)[[2]][[i]]
df.Paul$RSquared[i]<- summary(lm(as.formula(paste("saleprice ~", labels(train_h.Paul)[[2]][[i]])),data=train_h.Paul))$r.squared
}
# show the top 20
head(df.Paul[order(df.Paul$RSquared,decreasing=TRUE),],20)
train_h.Paul$neighborhood <- factor(train_h.Paul$neighborhood, levels = c("MeadowV" ,"IDOTRR" ,"BrDale" ,"OldTown" ,"BrkSide" ,"Edwards" ,"SWISU" ,"Sawyer" ,"NPkVill" ,"Blueste" ,"NAmes" ,"Mitchel" ,"SawyerW" ,"NWAmes" ,"Gilbert" ,"Greens" ,"Blmngtn" ,"CollgCr" ,"Crawfor" ,"ClearCr" ,"Somerst" ,"Timber" ,"Veenker" ,"NridgHt" ,"StoneBr" ,"NoRidge"))
test_h.Paul$neighborhood <- factor(test_h.Paul$neighborhood, levels = c("MeadowV" ,"IDOTRR" ,"BrDale" ,"OldTown" ,"BrkSide" ,"Edwards" ,"SWISU" ,"Sawyer" ,"NPkVill" ,"Blueste" ,"NAmes" ,"Mitchel" ,"SawyerW" ,"NWAmes" ,"Gilbert" ,"Greens" ,"Blmngtn" ,"CollgCr" ,"Crawfor" ,"ClearCr" ,"Somerst" ,"Timber" ,"Veenker" ,"NridgHt" ,"StoneBr" ,"NoRidge"))
ggplot(train_h.Paul, aes(x = neighborhood, y = saleprice)) +
geom_boxplot(fill = "#81AC9B") +
theme(axis.text.x = element_text(angle = 90, size = 8), legend.position = "none") +
scale_y_continuous("Sale Price", labels = dollar) +
scale_x_discrete("Neighborhood")
# In order to better interpret the regression analysis, factor variables are reordered and converted to numeric types relative to their order. For example:
# no fireplace(1) > unfinished fireplace (2) > finished fireplace (3)
test_h.Paul$bsmt.qual = as.numeric(factor(test_h.Paul$bsmt.qual, levels = c("No Basement", "Po", "Fa", "TA", "Gd", "Ex")))
train_h.Paul$bsmt.qual = as.numeric(factor(train_h.Paul$bsmt.qual, levels = c("No Basement", "Po", "Fa", "TA", "Gd", "Ex")))
# convert
test_h.Paul$exter.qual = as.numeric(factor(test_h.Paul$exter.qual, levels = c("Po", "Fa", "TA", "Gd", "Ex")))
train_h.Paul$exter.qual = as.numeric(factor(train_h.Paul$exter.qual, levels = c("Po", "Fa", "TA", "Gd", "Ex")))
# convert
test_h.Paul$kitchen.qual = as.numeric(factor(test_h.Paul$kitchen.qual, levels = c("Po", "Fa", "TA", "Gd", "Ex")))
train_h.Paul$kitchen.qual = as.numeric(factor(train_h.Paul$kitchen.qual, levels = c("Po", "Fa", "TA", "Gd", "Ex")))
# convert
test_h.Paul$garage.finish = as.numeric(factor(test_h.Paul$garage.finish, levels = c("None", "Unf", "RFn", "Fin")))
train_h.Paul$garage.finish = as.numeric(factor(train_h.Paul$garage.finish, levels = c("None", "Unf", "RFn", "Fin")))
# convert
test_h.Paul$fireplace.qu = as.numeric(factor(test_h.Paul$fireplace.qu, levels = c("No Fireplace", "Po", "Fa", "TA", "Gd", "Ex")))
train_h.Paul$fireplace.qu = as.numeric(factor(train_h.Paul$fireplace.qu, levels = c("No Fireplace", "Po", "Fa", "TA", "Gd", "Ex")))
#model 1
model.Paul<-lm(saleprice~neighborhood +
bsmt.qual +
total.bsmt.sf +
gr.liv.area +
exter.qual +
kitchen.qual +
garage.area +
garage.finish +
fireplace.qu +
yrs.since.remod +
ln.lot.area
, data = train_h.Paul )
# model 2
model.Paul2<-lm(saleprice~overall.qual +
neighborhood +
bsmt.qual +
total.bsmt.sf +
gr.liv.area +
exter.qual +
kitchen.qual +
garage.area +
fireplace.qu +
yrs.since.remod , data = train_h.Paul )
# regression output as table
stargazer(model.Paul, model.Paul2, type="html", style="apsr", digits=2, header=FALSE)
# Calculate test rmse
p1.Paul <- predict(model.Paul, test_h.Paul)
d.Paul.p1 <- data.frame(p1.Paul, test_h.Paul$saleprice)
saleprice_MSE.1 <- mean((d.Paul.p1[,1]-d.Paul.p1[,2])^2, na.rm=TRUE)
saleprice_RMSE_Paul.1 <- sqrt(saleprice_MSE.1)
# Calculate test rmse
Paul2 <- predict(model.Paul2, test_h.Paul)
dPaul.p2 <- data.frame(Paul2, test_h.Paul$saleprice)
saleprice_MSE.1 <- mean((dPaul.p2[,1]-dPaul.p2[,2])^2, na.rm=TRUE)
saleprice_RMSE_Paul.2 <- sqrt(saleprice_MSE.1)
rm(list=setdiff(ls(), c("saleprice_RMSE_Paul.1", "saleprice_RMSE_Paul.2", "RMSE.forward", "RMSE.back", "RMSE.tree", "RMSE.1", "RMSE.2", "RMSE.all")))
df_train <- read.csv("train_housing.csv", header = TRUE)
df_train_num <- df_train
df_test <- read.csv("test_housing.csv", header = TRUE)
# drop factors
for(i in colnames(df_train)){
if(is.factor( df_train[[i]] )){
df_train_num[[i]] <- NULL
}
}
pcr.saleprice <- pcr(saleprice ~ ., data=df_train_num, scale = TRUE, validation = "CV")
validationplot(pcr.saleprice)
summary(pcr.saleprice)
# predict RMSE pcr1
pcr.predict.1 <- predict(pcr.saleprice, df_test, ncomp=1)
df_predict.1 <- data.frame(pcr.predict.1, df_test$saleprice)
saleprice_MSE.1 <- mean((df_predict.1[,1]-df_predict.1[,2])^2)
saleprice_RMSE.1 <- sqrt(saleprice_MSE.1)
# predict RMSE pcr2
pcr.predict.2 <- predict(pcr.saleprice, df_test, ncomp=7)
df_predict.2 <- data.frame(pcr.predict.2, df_test$saleprice)
saleprice_MSE.2 <- mean((df_predict.2[,1]-df_predict.2[,2])^2)
saleprice_RMSE.2 <- sqrt(saleprice_MSE.2)
# predict RMSE pcr3
pcr.predict.3 <- predict(pcr.saleprice, df_test, ncomp=24)
df_predict.3 <- data.frame(pcr.predict.3, df_test$saleprice)
saleprice_MSE.3 <- mean((df_predict.3[,1]-df_predict.3[,2])^2)
saleprice_RMSE.3 <- sqrt(saleprice_MSE.3)
# new df
output_df <- data.frame(df_predict.1$df_test.saleprice, df_predict.1$saleprice.1.comps, df_predict.2$saleprice.7.comps, df_predict.3$saleprice.24.comps)
# rename df
output_df <- dplyr::rename(output_df, test_data=df_predict.1.df_test.saleprice, Prediction.1.Component=df_predict.1.saleprice.1.comps, Prediction.7.Component=df_predict.2.saleprice.7.comps, Prediction.24.Component=df_predict.3.saleprice.24.comps)
# output to table
stargazer(output_df, type="html", header=FALSE, summary.stat=c("n","median", "sd", "min", "max"), digits=2)
rm(list=setdiff(ls(), c("saleprice_RMSE.1", "saleprice_RMSE.2", "saleprice_RMSE.3", "saleprice_RMSE_Paul.1", "saleprice_RMSE_Paul.2", "RMSE.forward", "RMSE.back", "RMSE.tree", "RMSE.1", "RMSE.2", "RMSE.all")))
# read in train and test data
train_mike <- read.csv("train_housing.csv", header = TRUE)
test_mike <- read.csv("test_housing.csv", header = TRUE)
# make a dataframe of all numeric features
num_features = names(which(sapply(train_mike, is.numeric)))
df.numeric = train_mike[num_features]
# make a dataframe of all categorical features
cat_features = names(which(sapply(train_mike, is.factor)))
# convert all categorical features to numeric variables for modeling
train_mike[cat_features] <- sapply(train_mike[cat_features], as.integer)
test_mike[cat_features] <- sapply(test_mike[cat_features], as.integer)
# split test into x and y
test_x <- test_mike %>% dplyr::select(-saleprice)
test_y <- test_mike %>% dplyr::select(saleprice)
# set seed for reproducibility
set.seed(256)
# create 5 folds to be used in cross validation
myFolds <- createFolds(train_mike, k = 5)
# create a custom trainControl object to use our folds; index = myFolds
myControl = trainControl(verboseIter = FALSE, index = myFolds)
set.seed(12)
# train the model
modelrf1 <- train(
# formula
saleprice ~ .,
# data
train_mike,
# fast random forest
method = "ranger",
importance = "impurity",
# grid search for optimal number of columns to randomly sample at each split
tuneGrid = data.frame(mtry = seq(2, 80, 2)),
# set trainCrol as our custom object
trControl = myControl
)
# print the model
print(modelrf1)
# plot the number of randomly selected predictors
plot(modelrf1, xlab = "Number of Randomly Selected Predictors")
# plot 20 most important predictors of modelrf1
plot(varImp(modelrf1), top = 20)
set.seed(1760)
# train the model
modelrf2 <- train(
# formula
saleprice ~ .,
# data
train_mike,
# fast random forest
method = "ranger",
# grid search for optimal number of columns to randomly sample at each split
tuneGrid = data.frame(mtry = seq(2, 40, 2)),
# preprocess by removing zero variance predictors
preProcess = "zv",
# set trainCrol as our custom object
trControl = myControl
)
set.seed(17)
# train the model
modelrf3 <- train(
# formula
saleprice ~ .,
# data
train_mike,
# fast random forest
method = "ranger",
# grid search for optimal number of columns to randomly sample at each split
tuneGrid = data.frame(mtry = seq(1, 10, 1)),
# preProcess
preProcess = c("nzv", "center", "scale", "pca"),
# set trainCrol as our custom object
trControl = myControl
)
# print the model summary
print(modelrf3)
set.seed(500)
# Train glmnet with custom trainControl and tuning: model
enet1 <- train(
# formula
saleprice ~ .,
# data
train_mike,
# set grid search parameters for alpha
tuneGrid = expand.grid(alpha = seq(0,1,0.1),
lambda = seq(0.0001, 1, 10)),
# use glmnet method for lasso, ridge, and elastic net
method = "glmnet",
# trainControl
trControl = myControl
)
# Print model output to console
print(enet1)
# plot RMSE vs alpha
plot(enet1)
set.seed(1267)
# Train glmnet with custom trainControl and tuning: model
ridge <- train(
# formula
saleprice ~ .,
# data
train_mike,
# set grid search parameters for lambda
tuneGrid = expand.grid(alpha = 1,
lambda = (0:15) * 1000),
# use glmnet method for lasso, ridge, and elastic net
method = "glmnet",
# trainControl
trControl = myControl
)
# Print model output to console
print(ridge)
# plot RMSE vs lambda
plot(ridge)
set.seed(579)
# Train glmnet with custom trainControl and tuning: model
knn1 <- train(
# formula
saleprice ~ .,
# data
train_mike,
# knn regression
method = "kknn",
# trainControl
trControl = myControl
)
# Print model to console
print(knn1)
plot(knn1, xlab = "Maximum Number of Neighbors")
glmBoostGrid1 = expand.grid(mstop = seq(10, 150, 10),
prune = c("no"))
set.seed(1254)
# Train glmnet with custom trainControl and tuning: model
glmboost1 <- train(
# formula
saleprice ~ .,
# data
train_mike,
tuneGrid = glmBoostGrid1,
# gradient boosting tree
method = "glmboost",
# trainControl
trControl = myControl
)
# Print model to console
print(glmboost1)
# plot RMSE vs alpha
plot(glmboost1, xlab = "Number of Boosting Iterations")
# Create model_list
model_list <- list(rf1= modelrf1, rf2 = modelrf2, rf3 = modelrf3, `elastic net` = enet1,
ridge = ridge, knn = knn1, glmboost = glmboost1)
# Pass model_list to resamples(): resamples
resamps <- resamples(model_list)
# Summarize the results
summary(resamps)
bwplot(resamps, metric = "RMSE")
bwplot(resamps, metric = "Rsquared")
# A function to calculate the RMSE for a given model. The function takes 2 inputs: prediction and actual
# prediction should be a vector of predicted values and actual should be the vector of actual values
rmse <- function(prediction, actual) {
# calculate error
error <- prediction - actual
# calculate RMSE
sqrt(mean(error^2))
}
# Calculate model predictions on test data
pred_rf1 <- predict(modelrf1, test_x)
pred_rf2 <- predict(modelrf2, test_x)
pred_rf3 <- predict(modelrf3, test_x)
pred_enet <- predict(enet1, test_x)
pred_ridge <- predict(ridge, test_x)
pred_knn <- predict(knn1, test_x)
pred_glmboost <- predict(glmboost1, test_x)
# Calculate test RMSE
RMSE_rf1 <- rmse(pred_rf1, test_y)
RMSE_rf2 <- rmse(pred_rf2, test_y)
RMSE_rf3 <- rmse(pred_rf3, test_y)
RMSE_enet <- rmse(pred_enet, test_y)
RMSE_ridge <- rmse(pred_ridge, test_y)
RMSE_knn <- rmse(pred_knn, test_y)
RMSE_glmboost <- rmse(pred_glmboost, test_y)
df_test <- read.csv("test_housing.csv", header = TRUE)
prediction_rf1_test <- data.frame(df_test$saleprice, pred_rf1)
prediction_rf1_test <- dplyr::rename(prediction_rf1_test, Actual_SalePrice=df_test.saleprice, modelrf1_SalePrice=pred_rf1)
# output to table
stargazer(prediction_rf1_test, type="html", title="Table 5: Prediction Results", header=FALSE, summary.stat=c("n","median", "sd", "min", "max"), digits=2)
test_x[22,]
update_test_x_1 <- test_x[22,]
update_test_x_1$garage.cars <- 3
# plot RMSE vs alpha
ggplot(data=prediction_rf1_test, aes(x=Actual_SalePrice, y=modelrf1_SalePrice)) + geom_point(alpha=0.2, color="#00b9be") +
# plot RMSE vs alpha
ggplot(data=prediction_rf1_test, aes(x=Actual_SalePrice, y=modelrf1_SalePrice)) + geom_point(alpha=0.2, color="#00b9be")
# plot RMSE vs alpha
ggplot(data=prediction_rf1_test, aes(x=Actual_SalePrice, y=modelrf1_SalePrice)) + geom_point(alpha=0.2, color="#00b9be") + geom_abline()
ggplot(data=prediction_rf1_test, aes(x=Actual_SalePrice, y=modelrf1_SalePrice)) + geom_point(alpha=0.2, color="#00b9be") + geom_abline() + geom_smooth()
ggplot(data=prediction_rf1_test, aes(x=Actual_SalePrice, y=modelrf1_SalePrice)) + geom_point(alpha=0.2, color="#00b9be") + geom_abline() + geom_smooth(method="lm")
ggplot(data=prediction_rf1_test, aes(x=Actual_SalePrice, y=modelrf1_SalePrice)) + geom_point(alpha=0.2, color="#00b9be") + geom_abline() + geom_smooth(method="lm", color="#00b9be")
knitr::opts_chunk$set(echo = TRUE)
# packages needed for modeling
# install.packages("e1071")
# install.packages("ranger")
# install.packages("png")
# install.packages("grid")
# install.packages("glmnet")
# install.packages("kknn")
# install.packages("mboost")
# install.packages("htmlTable")
library(tidyverse)
library(stargazer)
library(pls)
library(scales)
library(car)
library(caret)
library(png)
library(grid)
library(lattice)
library(e1071)
library(ranger)
library(htmlTable)
options(scipen = 999)
df <- read.csv("cleaned_housing.csv", header = TRUE)
getwd()
setwd("Documents/GWWorkspace/DATS_Final")
getwd()
knitr::opts_chunk$set(echo = TRUE)
# packages needed for modeling
# install.packages("e1071")
# install.packages("ranger")
# install.packages("png")
# install.packages("grid")
# install.packages("glmnet")
# install.packages("kknn")
# install.packages("mboost")
# install.packages("htmlTable")
library(tidyverse)
library(stargazer)
library(pls)
library(scales)
library(car)
library(caret)
library(png)
library(grid)
library(lattice)
library(e1071)
library(ranger)
library(htmlTable)
options(scipen = 999)
df <- read.csv("cleaned_housing.csv", header = TRUE)
